{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7046579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")               \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector as selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, cross_validate, cross_val_predict\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "from shap import TreeExplainer\n",
    "from scipy import sparse\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, \n",
    "    classification_report, confusion_matrix,\n",
    "    confusion_matrix, roc_auc_score, \n",
    "    average_precision_score, precision_recall_curve,\n",
    "    precision_recall_fscore_support, auc\n",
    ")\n",
    "\n",
    "import importlib\n",
    "import fonctions.fonctions_features as ff\n",
    "importlib.reload(ff)\n",
    "from fonctions.fonctions_features import FeatureEngineer, ColumnDropper\n",
    "\n",
    "import fonctions.fonctions_CV as funcCV\n",
    "importlib.reload(funcCV)\n",
    "from fonctions.fonctions_CV import perm_importance_cv, get_transformed_feature_names_and_source_map\n",
    "\n",
    "import fonctions.fonctions_grid as fg\n",
    "importlib.reload(fg)\n",
    "from fonctions.fonctions_grid import get_positive_scores, evaluate_at_threshold, pick_threshold, pick_threshold_oof"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f72c43",
   "metadata": {},
   "source": [
    "### Import du jeu de données \n",
    " Dans cette partie j'importe les données et je parametre des élèments qui sont utilisés dans plusieurs parties du code comme les élèments à supprimer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f052b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/jeu_donnee_RH_complet_transforme.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae62c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code réutilisé dans l'ensemble des cellules\n",
    "\n",
    "cols_a_supprimer = [\n",
    "    \"hors_entreprise_majoritaire\",\n",
    "    \"perf_degrade_flag\",\n",
    "    \"perf_degrade_niv\",\n",
    "    \"a_connu_mvmnt_interne\",\n",
    "    \"genre\",\n",
    "    \"heure_supplementaires\",\n",
    "    \"pee_participation_flag\",\n",
    "    \"pee_participation_2plus\",\n",
    "    \"revenu_mensuel\",\n",
    "    \"niveau_education\",\n",
    "    \"niveau_hierarchique_poste\",\n",
    "    \"annees_dans_le_poste_actuel\",\n",
    "    \"poste\",\n",
    "    \"annees_dans_l_entreprise\",\n",
    "    \"age\",\n",
    "    \"annee_experience_totale\",\n",
    "    \"annes_sous_responsable_actuel\",\n",
    "]\n",
    "\n",
    "\n",
    "thr = 0.3\n",
    "\n",
    "over = SMOTE(sampling_strategy=0.2, k_neighbors=5)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e0005",
   "metadata": {},
   "source": [
    "# Test des différents modèles sur une version basique pour selectionner le modèle\n",
    "\n",
    "Cette partie du code lance différents modèles avec la même méthodologie pour tenter de choisir le modèle qui aura les performances les plus prometteuses. \n",
    "Le modèle Dummy sert d'étalon. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c106d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Dummy ===\n",
      "[precision] train=0.000±0.000 | test=0.000±0.000\n",
      "[recall] train=0.000±0.000 | test=0.000±0.000\n",
      "[roc_auc] train=0.500±0.000 | test=0.500±0.000\n",
      "\n",
      "-- RAPPORT (seuil à 0.3) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.840     1.000     0.913       247\n",
      "           1      0.000     0.000     0.000        47\n",
      "\n",
      "    accuracy                          0.840       294\n",
      "   macro avg      0.420     0.500     0.457       294\n",
      "weighted avg      0.706     0.840     0.767       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[247   0]\n",
      " [ 47   0]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.500\n",
      "PR AUC  (test) = 0.160\n"
     ]
    }
   ],
   "source": [
    "#DUMMY\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_dummy = Pipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- ton feature engineering, sans fuite\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", DummyClassifier(strategy=\"most_frequent\", random_state=42)),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"clas\n",
    "# sique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_dummy.fit(X_train, y_train) \n",
    "\n",
    "y_pred_dummy = pipe_dummy.predict(X_test)\n",
    "\n",
    "# --- 4B) En Cross-Validation stratifiée (pas de fuite) ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_dummy, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Dummy ===\")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_dummy.predict_proba(X_test)[:, 1]\n",
    "y_pred_06 = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(f\"\\n-- RAPPORT (seuil à {thr}) --\")\n",
    "print(classification_report(y_test, y_pred_06, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_06))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2886e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport RegressionLogistique\n",
      "[precision] train=0.408±0.010 | test=0.380±0.016\n",
      "[recall] train=0.801±0.012 | test=0.772±0.033\n",
      "[roc_auc] train=0.867±0.004 | test=0.832±0.014\n",
      "\n",
      "-- RAPPORT (seuil à 0.3) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.958     0.559     0.706       247\n",
      "           1      0.273     0.872     0.416        47\n",
      "\n",
      "    accuracy                          0.609       294\n",
      "   macro avg      0.616     0.716     0.561       294\n",
      "weighted avg      0.849     0.609     0.660       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[138 109]\n",
      " [  6  41]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.826\n",
      "PR AUC  (test) = 0.548\n"
     ]
    }
   ],
   "source": [
    "#RegressionLogistique\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_logit = ImbPipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "    (\"prep\", preprocess),\n",
    "    (\"over\", over),\n",
    "    (\"under\", under),\n",
    "    (\"clf\", LogisticRegression(class_weight=\"balanced\", max_iter=2000)),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_logit.fit(X_train, y_train)\n",
    "yp_logit_proba = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "yp_logit = (yp_logit_proba >= thr).astype(int)\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_logit, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Rapport RegressionLogistique\")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_logit.predict_proba(X_test)[:, 1]\n",
    "y_pred_thr = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(f\"\\n-- RAPPORT (seuil à {thr}) --\")\n",
    "print(classification_report(y_test, y_pred_thr, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_thr))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5488b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport ForetAleatoire\n",
      "[precision] train=0.779±0.023 | test=0.504±0.040\n",
      "[recall] train=1.000±0.000 | test=0.473±0.027\n",
      "[roc_auc] train=0.997±0.001 | test=0.809±0.024\n",
      "\n",
      "-- RAPPORT (seuil à 0.3) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.953     0.575     0.717       247\n",
      "           1      0.276     0.851     0.417        47\n",
      "\n",
      "    accuracy                          0.619       294\n",
      "   macro avg      0.614     0.713     0.567       294\n",
      "weighted avg      0.845     0.619     0.669       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[142 105]\n",
      " [  7  40]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.810\n",
      "PR AUC  (test) = 0.518\n"
     ]
    }
   ],
   "source": [
    "#ForetAleatoire\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_random = ImbPipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "    (\"prep\", preprocess),\n",
    "    (\"over\", over),\n",
    "    (\"under\", under),\n",
    "    (\"rf\", RandomForestClassifier(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_random.fit(X_train, y_train)\n",
    "yp_random = pipe_random.predict(X_test)\n",
    "yp_random_proba = pipe_random.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_random, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"Rapport ForetAleatoire\")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_random.predict_proba(X_test)[:, 1]\n",
    "y_pred_thr = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(f\"\\n-- RAPPORT (seuil à {thr}) --\")\n",
    "print(classification_report(y_test, y_pred_thr, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_thr))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1e3a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport HistGradientBoosting\n",
      "[precision] train=0.704±0.027 | test=0.465±0.039\n",
      "[recall] train=0.848±0.052 | test=0.515±0.068\n",
      "[roc_auc] train=0.952±0.009 | test=0.793±0.017\n",
      "\n",
      "-- RAPPORT (seuil à 0.3) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.941     0.717     0.814       247\n",
      "           1      0.340     0.766     0.471        47\n",
      "\n",
      "    accuracy                          0.724       294\n",
      "   macro avg      0.641     0.741     0.642       294\n",
      "weighted avg      0.845     0.724     0.759       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[177  70]\n",
      " [ 11  36]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.802\n",
      "PR AUC  (test) = 0.571\n"
     ]
    }
   ],
   "source": [
    "#HistGradientBoosting\n",
    "\n",
    "# --- 2) Préparation des données d’entrée \n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_sel = selector(dtype_include=[\"number\", \"bool\"])   # inclut les bools comme numériques\n",
    "categorical_sel = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())   # centrage + mise à l'échelle des numériques\n",
    "        ]), numeric_sel),\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]), categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_hgb = ImbPipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- feature engineering, sans fuite\n",
    "    (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "    (\"prep\", preprocess),\n",
    "    (\"over\", over),\n",
    "    (\"under\", under),    \n",
    "    (\"hgb\", HistGradientBoostingClassifier(\n",
    "    max_depth=None,\n",
    "    learning_rate=0.1,\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    "    # (pas de class_weight ici; on compensera avec la métrique choisie et, plus tard, le tuning)\n",
    ")),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_hgb.fit(X_train, y_train)\n",
    "yp_hgb = pipe_hgb.predict(X_test)\n",
    "yp_hgb_proba = pipe_hgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_hgb, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "print(\"Rapport HistGradientBoosting\")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_hgb.predict_proba(X_test)[:, 1]\n",
    "y_pred_thr = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(f\"\\n-- RAPPORT (seuil à {thr}) --\")\n",
    "print(classification_report(y_test, y_pred_thr, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_thr))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71b867af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XgBoost ===\n",
      "[precision] train=0.730±0.013 | test=0.459±0.031\n",
      "[recall] train=1.000±0.000 | test=0.510±0.050\n",
      "[roc_auc] train=0.996±0.001 | test=0.803±0.012\n",
      "\n",
      "-- RAPPORT (seuil à 0.3) --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.925     0.802     0.859       247\n",
      "           1      0.388     0.660     0.488        47\n",
      "\n",
      "    accuracy                          0.779       294\n",
      "   macro avg      0.656     0.731     0.674       294\n",
      "weighted avg      0.839     0.779     0.800       294\n",
      "\n",
      "-- MATRICE DE CONFUSION --\n",
      "[[198  49]\n",
      " [ 16  31]]\n",
      "-- AUCs (seuil-indep.) --\n",
      "ROC AUC (test) = 0.813\n",
      "PR AUC  (test) = 0.485\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST\n",
    "# --- 2) Préparation des données d’entrée (pandas) ---\n",
    "data_pd = data.copy()\n",
    "\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "# --- 3) Pipeline complète avec TON FE + préprocessing classique + XGB ---\n",
    "numeric_cont_sel = selector(dtype_include=[\"number\"], dtype_exclude=[\"bool\"])\n",
    "bool_sel         = selector(dtype_include=[\"bool\"])\n",
    "categorical_sel  = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "num_poly_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    #(\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "bool_pipe = Pipeline([\n",
    "    (\"to_int\", FunctionTransformer(lambda X: X.astype(np.int8))),   # <- bool → 0/1\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # dense\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  num_poly_pipe, numeric_cont_sel),\n",
    "        (\"bool\", bool_pipe, bool_sel),\n",
    "        (\"cat\",  cat_pipe, categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "pipe_XG = ImbPipeline(steps=[\n",
    "    (\"fe\", FeatureEngineer()),   # <- ton feature engineering, sans fuite\n",
    "    (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "    (\"prep\", preprocess),\n",
    "    (\"over\", over),\n",
    "    (\"under\", under),    \n",
    "    (\"xgb\", XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        n_jobs=-1\n",
    "    )),\n",
    "])\n",
    "\n",
    "# --- 4A) En TRAIN / TEST \"classique\" (pas de fuite) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "pipe_XG.fit(X_train, y_train)              \n",
    "\n",
    "# --- 4B) En Cross-Validation stratifiée (pas de fuite) ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = (\"precision\", \"recall\", \"roc_auc\")\n",
    "scores = cross_validate(\n",
    "    pipe_XG, X, y, cv=cv,\n",
    "    scoring=(\"precision\", \"recall\", \"roc_auc\"),\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== XgBoost ===\")\n",
    "\n",
    "for m in scoring:\n",
    "    tr_mean, tr_std = scores[f\"train_{m}\"].mean(), scores[f\"train_{m}\"].std()\n",
    "    te_mean, te_std = scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()\n",
    "    print(f\"[{m}] train={tr_mean:.3f}±{tr_std:.3f} | test={te_mean:.3f}±{te_std:.3f}\")\n",
    "\n",
    "proba_te = pipe_XG.predict_proba(X_test)[:, 1]\n",
    "y_pred_06 = (proba_te >= thr).astype(int)\n",
    "\n",
    "print(f\"\\n-- RAPPORT (seuil à {0.3}) --\")\n",
    "print(classification_report(y_test, y_pred_06, digits=3, zero_division=0))\n",
    "\n",
    "print(\"-- MATRICE DE CONFUSION --\")\n",
    "print(confusion_matrix(y_test, y_pred_06))  # [[tn, fp], [fn, tp]]\n",
    "\n",
    "print(\"-- AUCs (seuil-indep.) --\")\n",
    "print(f\"ROC AUC (test) = {roc_auc_score(y_test, proba_te):.3f}\")\n",
    "print(f\"PR AUC  (test) = {average_precision_score(y_test, proba_te):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21885540",
   "metadata": {},
   "source": [
    "# Calibrage des hyperparametres \n",
    "\n",
    "Cette partie du code calibre les hyperparametres. \n",
    "Deux solutions pour le rééquilibrage des données sont testé (SMOTE et scale_pos_weight) dans deux code similaires\n",
    "Un des deux codes sera à retirer selon les résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839a861a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n",
      "\n",
      "=== Meilleurs hyperparamètres (refit=AP) ===\n",
      "{'xgb__colsample_bytree': 0.7, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 3, 'xgb__min_child_weight': 3, 'xgb__n_estimators': 400, 'xgb__reg_alpha': 0.1, 'xgb__reg_lambda': 1.0, 'xgb__subsample': 0.7}\n",
      "Meilleure AP (CV) : 0.5788\n",
      "\n",
      "=== Seuil choisi (OOF) ===\n",
      "{'chosen_mode': 'target_recall', 'recall_target': 0.75, 'threshold': 0.2880828082561493, 'precision': 0.35049019607843135, 'recall': 0.7526315789473684, 'f1': 0.4782608695652174, 'avg_precision_pr': 0.5631597784050519, 'roc_auc': 0.8189708551297107}\n",
      "\n",
      "=== AUC/AP sur test (seuil-indépendant) ===\n",
      "ROC AUC (test) : 0.795\n",
      "Avg Precision (test) : 0.536\n",
      "\n",
      "=== Rapport @ seuil OOF (target_recall) ===\n",
      "Seuil: 0.2881\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.948     0.733     0.826       247\n",
      "           1      0.359     0.787     0.493        47\n",
      "\n",
      "    accuracy                          0.741       294\n",
      "   macro avg      0.653     0.760     0.660       294\n",
      "weighted avg      0.854     0.741     0.773       294\n",
      "\n",
      "Confusion matrix [[tn, fp],[fn, tp]]:\n",
      " [[181  66]\n",
      " [ 10  37]]\n",
      "\n",
      "[INFO] Test F1-max (diagnostic) : thr=0.4629 | P=0.483 R=0.596 F1=0.533\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# XGBoost + (option) SMOTE/Under + GridSearch + Seuil PR + (option) Calibration\n",
    "# ===============================================================\n",
    "\n",
    "# ===============================================================\n",
    "# 0) Paramètres généraux à ajuster\n",
    "# ===============================================================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Choix de stratégie d'équilibrage\n",
    "USE_SMOTE = True          # True => SMOTE+UnderSampler ; False => pas de resampling\n",
    "SMOTE_RATIO = 0.20        # ratio cible (minorité / majorité) pour SMOTE\n",
    "UNDER_RATIO = 0.50        # ratio cible (minorité / majorité) après under-sampling\n",
    "\n",
    "# Calibration des probabilités (isotonic ou sigmoid). Mets True si tu veux calibrer\n",
    "USE_CALIBRATION = False\n",
    "CALIB_METHOD = \"isotonic\"  # \"isotonic\" (plus souple) ou \"sigmoid\" (Platt)\n",
    "\n",
    "# Objectif business (seuil)\n",
    "USE_TARGET_RECALL = True   # True => choisir un seuil pour atteindre un rappel cible\n",
    "RECALL_TARGET = 0.75       # rappel cible\n",
    "# Si False => on prendra le seuil qui maximise F1 (sur OOF)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3) Données & split\n",
    "# ===============================================================\n",
    "# On part d'un DataFrame `data` avec une cible 'a_quitte_l_entreprise' (\"Non\"/\"Oui\")\n",
    "data_pd = data.copy()\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 4) Préprocessing (imputer+scale num, imputer+OHE cat)\n",
    "#     NB: on laisse OHE même pour XGBoost pour rester simple & compatible SMOTE\n",
    "# ===============================================================\n",
    "numeric_cont_sel = selector(dtype_include=[\"number\"], dtype_exclude=[\"bool\"])\n",
    "bool_sel         = selector(dtype_include=[\"bool\"])\n",
    "categorical_sel  = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "num_poly_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    #(\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "bool_pipe = Pipeline([\n",
    "    (\"to_int\", FunctionTransformer(lambda X: X.astype(np.int8),\n",
    "                                  feature_names_out=\"one-to-one\")),   # <- bool → 0/1\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # dense\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  num_poly_pipe, numeric_cont_sel),\n",
    "        (\"bool\", bool_pipe, bool_sel),\n",
    "        (\"cat\",  cat_pipe, categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 5) XGBClassifier + (option) SMOTE/Under + GridSearchCV\n",
    "# ===============================================================\n",
    "# Option alternative au SMOTE: scale_pos_weight = (#neg/#pos) recommandé par XGBoost\n",
    "def compute_scale_pos_weight(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    return float(neg) / float(pos) if pos > 0 else 1.0\n",
    "\n",
    "base_xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",          # pertinent quand classes déséquilibrées\n",
    "    tree_method=\"hist\",           # rapide\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "if not USE_SMOTE:\n",
    "    base_xgb_params[\"scale_pos_weight\"] = compute_scale_pos_weight(y_train)  # cf. doc XGBoost\n",
    "\n",
    "xgb_clf = XGBClassifier(**base_xgb_params)\n",
    "\n",
    "if USE_SMOTE:\n",
    "    # IMPORTANT : imblearn.Pipeline pour chaîner des resamplers\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"fe\", FeatureEngineer()),\n",
    "        (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "        (\"prep\", preprocess),\n",
    "        (\"over\", SMOTE(sampling_strategy=SMOTE_RATIO, k_neighbors=5, random_state=RANDOM_STATE)),\n",
    "        (\"under\", RandomUnderSampler(sampling_strategy=UNDER_RATIO, random_state=RANDOM_STATE)),\n",
    "        (\"xgb\", xgb_clf),\n",
    "    ])\n",
    "else:\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"fe\", FeatureEngineer()),\n",
    "        (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", xgb_clf),\n",
    "    ])\n",
    "\n",
    "# Grille XGB \"raisonnable\" (pas trop grosse) — on refit sur AP (PR-AUC)\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [400, 800],\n",
    "    \"xgb__learning_rate\": [0.03, 0.08, 0.15],\n",
    "    \"xgb__max_depth\": [3, 4, 5],\n",
    "    \"xgb__min_child_weight\": [1, 3, 5],\n",
    "    \"xgb__subsample\": [0.7, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "    \"xgb__reg_alpha\": [0.0, 0.1],\n",
    "    \"xgb__reg_lambda\": [1.0, 2.0, 5.0],\n",
    "    # \"xgb__gamma\": [0.0, 0.1],  # à activer si tu veux durcir le split\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "scoring = {\"ap\": \"average_precision\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"ap\",            # on refit sur la meilleure AP (PR-AUC)\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"\\n=== Meilleurs hyperparamètres (refit=AP) ===\")\n",
    "print(gs.best_params_)\n",
    "print(f\"Meilleure AP (CV) : {gs.best_score_:.4f}\")\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# ===============================================================\n",
    "# 6) (option) Calibration des probabilités\n",
    "#     ⚠️ Idéalement faire la calibration sur un split de validation séparé.\n",
    "#     Ici on illustre avec cv=5 pour limiter la fuite.\n",
    "# ===============================================================\n",
    "if USE_CALIBRATION:\n",
    "    calibrator = CalibratedClassifierCV(\n",
    "        base_estimator=best_model,\n",
    "        method=CALIB_METHOD,\n",
    "        cv=5\n",
    "    )\n",
    "    calibrator.fit(X_train, y_train)\n",
    "    final_model = calibrator\n",
    "else:\n",
    "    final_model = best_model\n",
    "\n",
    "# ===============================================================\n",
    "# 7) Choix de seuil SANS regarder le test (OOF sur le TRAIN)\n",
    "# ===============================================================\n",
    "mode = \"target_recall\" if USE_TARGET_RECALL else \"max_f1\"\n",
    "thr_oof, thr_summary = pick_threshold_oof(\n",
    "    final_model, X_train, y_train,\n",
    "    mode=mode, recall_target=RECALL_TARGET,\n",
    "    n_splits=5, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"\\n=== Seuil choisi (OOF) ===\")\n",
    "print(thr_summary)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) Évaluation finale sur TEST\n",
    "# ===============================================================\n",
    "# refit final sur tout le TRAIN (GridSearchCV l’a déjà fait sur folds; on refit encore par sécurité)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Scores seuil-indépendants\n",
    "proba_test = get_positive_scores(final_model, X_test)\n",
    "print(\"\\n=== AUC/AP sur test (seuil-indépendant) ===\")\n",
    "print(\"ROC AUC (test) :\", f\"{roc_auc_score(y_test, proba_test):.3f}\")\n",
    "print(\"Avg Precision (test) :\", f\"{average_precision_score(y_test, proba_test):.3f}\")\n",
    "\n",
    "# Rapport au seuil OOF\n",
    "res = evaluate_at_threshold(y_test, proba_test, thr_oof)\n",
    "print(f\"\\n=== Rapport @ seuil OOF ({mode}) ===\")\n",
    "print(f\"Seuil: {res['threshold']:.4f}\")\n",
    "print(res[\"report\"])\n",
    "print(\"Confusion matrix [[tn, fp],[fn, tp]]:\\n\", res[\"confusion_matrix\"])\n",
    "\n",
    "# (Option diagnostic) point F1-max sur le TEST (ne PAS geler ce seuil en production)\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_test)\n",
    "P, R, T = prec[:-1], rec[:-1], thr\n",
    "F1 = (2 * P * R) / (P + R + 1e-12)\n",
    "ix = int(np.nanargmax(F1))\n",
    "print(\"\\n[INFO] Test F1-max (diagnostic) : \"\n",
    "      f\"thr={T[ix]:.4f} | P={P[ix]:.3f} R={R[ix]:.3f} F1={F1[ix]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3198226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2916 candidates, totalling 14580 fits\n",
      "\n",
      "=== Meilleurs hyperparamètres (refit=AP) ===\n",
      "{'xgb__colsample_bytree': 0.9, 'xgb__learning_rate': 0.03, 'xgb__max_depth': 3, 'xgb__min_child_weight': 5, 'xgb__n_estimators': 400, 'xgb__reg_alpha': 0.0, 'xgb__reg_lambda': 5.0, 'xgb__subsample': 0.7}\n",
      "Meilleure AP (CV) : 0.5894\n",
      "\n",
      "=== Seuil choisi (OOF) ===\n",
      "{'chosen_mode': 'target_recall', 'recall_target': 0.75, 'threshold': 0.1479661077260971, 'precision': 0.34375, 'recall': 0.7526315789473684, 'f1': 0.471947194719472, 'avg_precision_pr': 0.5781744669838718, 'roc_auc': 0.8145857798654852}\n",
      "\n",
      "=== AUC/AP sur test (seuil-indépendant) ===\n",
      "ROC AUC (test) : 0.819\n",
      "Avg Precision (test) : 0.564\n",
      "\n",
      "=== Rapport @ seuil OOF (target_recall) ===\n",
      "Seuil: 0.1480\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.953     0.737     0.831       247\n",
      "           1      0.369     0.809     0.507        47\n",
      "\n",
      "    accuracy                          0.748       294\n",
      "   macro avg      0.661     0.773     0.669       294\n",
      "weighted avg      0.860     0.748     0.779       294\n",
      "\n",
      "Confusion matrix [[tn, fp],[fn, tp]]:\n",
      " [[182  65]\n",
      " [  9  38]]\n",
      "\n",
      "[INFO] Test F1-max (diagnostic) : thr=0.2233 | P=0.485 R=0.681 F1=0.566\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# XGBoost + (option) SMOTE/Under + GridSearch + Seuil PR + (option) Calibration\n",
    "# ===============================================================\n",
    "\n",
    "# ===============================================================\n",
    "# 0) Paramètres généraux à ajuster\n",
    "# ===============================================================\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Choix de stratégie d'équilibrage\n",
    "USE_SMOTE = False          # True => SMOTE+UnderSampler ; False => pas de resampling\n",
    "SMOTE_RATIO = 0.20        # ratio cible (minorité / majorité) pour SMOTE\n",
    "UNDER_RATIO = 0.50        # ratio cible (minorité / majorité) après under-sampling\n",
    "\n",
    "# Calibration des probabilités (isotonic ou sigmoid). Mets True si tu veux calibrer\n",
    "USE_CALIBRATION = True\n",
    "CALIB_METHOD = \"isotonic\"  # \"isotonic\" (plus souple) ou \"sigmoid\" (Platt)\n",
    "\n",
    "# Objectif business (seuil)\n",
    "USE_TARGET_RECALL = True   # True => choisir un seuil pour atteindre un rappel cible\n",
    "RECALL_TARGET = 0.75       # rappel cible\n",
    "# Si False => on prendra le seuil qui maximise F1 (sur OOF)\n",
    "\n",
    "# ===============================================================\n",
    "# 1) Utilitaires (seuils & évaluations)\n",
    "# ===============================================================\n",
    "def get_positive_scores(model, X):\n",
    "    \"\"\"Retourne un score pour la classe positive (1): predict_proba[:,1] sinon decision_function.\"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        return model.predict_proba(X)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        if s.ndim == 1:\n",
    "            return s\n",
    "        # Multi-sorties: essaye d'attraper la colonne de la classe 1\n",
    "        pos_idx = -1\n",
    "        if hasattr(model, \"classes_\"):\n",
    "            cls = np.array(model.classes_)\n",
    "            idx = np.where(cls == 1)[0]\n",
    "            if len(idx):\n",
    "                pos_idx = int(idx[0])\n",
    "        return s[:, pos_idx]\n",
    "    else:\n",
    "        raise AttributeError(\"Le modèle ne fournit ni predict_proba ni decision_function.\")\n",
    "\n",
    "def evaluate_at_threshold(y_true, scores, thr):\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_true, y_pred)\n",
    "    ap = average_precision_score(y_true, scores)\n",
    "    roc = roc_auc_score(y_true, scores)\n",
    "    cm = confusion_matrix(y_true, y_pred)  # [[tn, fp],[fn, tp]]\n",
    "    rep = classification_report(y_true, y_pred, digits=3, zero_division=0)\n",
    "    return {\n",
    "        \"threshold\": float(thr),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1\": float(f1),\n",
    "        \"accuracy\": float(acc),\n",
    "        \"balanced_accuracy\": float(bacc),\n",
    "        \"avg_precision_pr\": float(ap),\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"report\": rep\n",
    "    }\n",
    "\n",
    "def pick_threshold(y_true, scores, mode=\"max_f1\", recall_target=0.75):\n",
    "    \"\"\"\n",
    "    Choisit un seuil depuis la courbe PR (F1-max ou rappel cible), via precision_recall_curve (sklearn).\n",
    "    \"\"\"\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, scores)\n",
    "    # Alignement: thresholds a une longueur len(precisions)-1\n",
    "    P, R, T = precisions[:-1], recalls[:-1], thresholds\n",
    "    F1 = np.where((P + R) > 0, 2 * P * R / (P + R), 0.0)\n",
    "\n",
    "    if mode == \"max_f1\":\n",
    "        idx = int(np.nanargmax(F1))\n",
    "    elif mode == \"target_recall\":\n",
    "        feas = np.where(R >= recall_target)[0]\n",
    "        idx = int(feas[np.nanargmax(F1[feas])]) if len(feas) else int(np.nanargmax(F1))\n",
    "    else:\n",
    "        raise ValueError(\"mode doit être 'max_f1' ou 'target_recall'\")\n",
    "\n",
    "    return float(T[idx]), {\n",
    "        \"chosen_mode\": mode,\n",
    "        \"recall_target\": recall_target if mode == \"target_recall\" else None,\n",
    "        \"threshold\": float(T[idx]),\n",
    "        \"precision\": float(P[idx]),\n",
    "        \"recall\": float(R[idx]),\n",
    "        \"f1\": float(F1[idx]),\n",
    "        \"avg_precision_pr\": float(average_precision_score(y_true, scores)),\n",
    "        \"roc_auc\": float(roc_auc_score(y_true, scores)),\n",
    "    }\n",
    "\n",
    "def pick_threshold_oof(model, X_train, y_train, mode=\"max_f1\", recall_target=0.75, n_splits=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Choix de seuil sur des scores OOF (out-of-fold) pour éviter toute fuite sur le test.\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    oof_scores = cross_val_predict(model, X_train, y_train, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "    thr, summary = pick_threshold(y_train, oof_scores, mode=mode, recall_target=recall_target)\n",
    "    return float(thr), summary\n",
    "\n",
    "# ===============================================================\n",
    "# 3) Données & split\n",
    "# ===============================================================\n",
    "# On part d'un DataFrame `data` avec une cible 'a_quitte_l_entreprise' (\"Non\"/\"Oui\")\n",
    "data_pd = data.copy()\n",
    "X = data_pd.drop(columns=[\"a_quitte_l_entreprise\"])\n",
    "y = data_pd[\"a_quitte_l_entreprise\"].map({\"Non\": 0, \"Oui\": 1}).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 4) Préprocessing (imputer+scale num, imputer+OHE cat)\n",
    "#     NB: on laisse OHE même pour XGBoost pour rester simple & compatible SMOTE\n",
    "# ===============================================================\n",
    "numeric_cont_sel = selector(dtype_include=[\"number\"], dtype_exclude=[\"bool\"])\n",
    "bool_sel         = selector(dtype_include=[\"bool\"])\n",
    "categorical_sel  = selector(dtype_exclude=[\"number\", \"bool\"])\n",
    "\n",
    "num_poly_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    #(\"poly\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "bool_pipe = Pipeline([\n",
    "    (\"to_int\", FunctionTransformer(lambda X: X.astype(np.int8),\n",
    "                                  feature_names_out=\"one-to-one\")),   # <- bool → 0/1\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))  # dense\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\",  num_poly_pipe, numeric_cont_sel),\n",
    "        (\"bool\", bool_pipe, bool_sel),\n",
    "        (\"cat\",  cat_pipe, categorical_sel),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 5) XGBClassifier + (option) SMOTE/Under + GridSearchCV\n",
    "# ===============================================================\n",
    "# Option alternative au SMOTE: scale_pos_weight = (#neg/#pos) recommandé par XGBoost\n",
    "def compute_scale_pos_weight(y):\n",
    "    pos = (y == 1).sum()\n",
    "    neg = (y == 0).sum()\n",
    "    return float(neg) / float(pos) if pos > 0 else 1.0\n",
    "\n",
    "base_xgb_params = dict(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"aucpr\",          # pertinent quand classes déséquilibrées\n",
    "    tree_method=\"hist\",           # rapide\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "if not USE_SMOTE:\n",
    "    base_xgb_params[\"scale_pos_weight\"] = compute_scale_pos_weight(y_train)  # cf. doc XGBoost\n",
    "\n",
    "xgb_clf = XGBClassifier(**base_xgb_params)\n",
    "\n",
    "if USE_SMOTE:\n",
    "    # IMPORTANT : imblearn.Pipeline pour chaîner des resamplers\n",
    "    pipe = ImbPipeline(steps=[\n",
    "        (\"fe\", FeatureEngineer()),\n",
    "        (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "        (\"prep\", preprocess),\n",
    "        (\"over\", SMOTE(sampling_strategy=SMOTE_RATIO, k_neighbors=5, random_state=RANDOM_STATE)),\n",
    "        (\"under\", RandomUnderSampler(sampling_strategy=UNDER_RATIO, random_state=RANDOM_STATE)),\n",
    "        (\"xgb\", xgb_clf),\n",
    "    ])\n",
    "else:\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"fe\", FeatureEngineer()),\n",
    "        (\"drop\", ColumnDropper(columns=cols_a_supprimer)),\n",
    "        (\"prep\", preprocess),\n",
    "        (\"xgb\", xgb_clf),\n",
    "    ])\n",
    "\n",
    "# Grille XGB \"raisonnable\" (pas trop grosse) — on refit sur AP (PR-AUC)\n",
    "param_grid = {\n",
    "    \"xgb__n_estimators\": [400, 800],\n",
    "    \"xgb__learning_rate\": [0.03, 0.08, 0.15],\n",
    "    \"xgb__max_depth\": [3, 4, 5],\n",
    "    \"xgb__min_child_weight\": [1, 3, 5],\n",
    "    \"xgb__subsample\": [0.7, 0.9, 1.0],\n",
    "    \"xgb__colsample_bytree\": [0.7, 0.9, 1.0],\n",
    "    \"xgb__reg_alpha\": [0.0, 0.1],\n",
    "    \"xgb__reg_lambda\": [1.0, 2.0, 5.0],\n",
    "    # \"xgb__gamma\": [0.0, 0.1],  # à activer si tu veux durcir le split\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "scoring = {\"ap\": \"average_precision\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit=\"ap\",            # on refit sur la meilleure AP (PR-AUC)\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "print(\"\\n=== Meilleurs hyperparamètres (refit=AP) ===\")\n",
    "print(gs.best_params_)\n",
    "print(f\"Meilleure AP (CV) : {gs.best_score_:.4f}\")\n",
    "\n",
    "best_model = gs.best_estimator_\n",
    "\n",
    "# ===============================================================\n",
    "# 6) (option) Calibration des probabilités\n",
    "#     ⚠️ Idéalement faire la calibration sur un split de validation séparé.\n",
    "#     Ici on illustre avec cv=5 pour limiter la fuite.\n",
    "# ===============================================================\n",
    "if USE_CALIBRATION:\n",
    "    \n",
    "    calibrator = CalibratedClassifierCV(\n",
    "        estimator=best_model,     # <-- et plus 'base_estimator'\n",
    "        method=CALIB_METHOD,      # \"isotonic\" ou \"sigmoid\"\n",
    "        cv=5,                     # CV interne pour calibrer\n",
    "        # n_jobs=-1               # dispo selon ta version de sklearn\n",
    "    )\n",
    "    calibrator.fit(X_train, y_train)\n",
    "    final_model = calibrator\n",
    "\n",
    "else:\n",
    "    final_model = best_model\n",
    "\n",
    "# ===============================================================\n",
    "# 7) Choix de seuil SANS regarder le test (OOF sur le TRAIN)\n",
    "# ===============================================================\n",
    "mode = \"target_recall\" if USE_TARGET_RECALL else \"max_f1\"\n",
    "thr_oof, thr_summary = pick_threshold_oof(\n",
    "    final_model, X_train, y_train,\n",
    "    mode=mode, recall_target=RECALL_TARGET,\n",
    "    n_splits=5, random_state=RANDOM_STATE\n",
    ")\n",
    "print(\"\\n=== Seuil choisi (OOF) ===\")\n",
    "print(thr_summary)\n",
    "\n",
    "# ===============================================================\n",
    "# 8) Évaluation finale sur TEST\n",
    "# ===============================================================\n",
    "# refit final sur tout le TRAIN (GridSearchCV l’a déjà fait sur folds; on refit encore par sécurité)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Scores seuil-indépendants\n",
    "proba_test = get_positive_scores(final_model, X_test)\n",
    "print(\"\\n=== AUC/AP sur test (seuil-indépendant) ===\")\n",
    "print(\"ROC AUC (test) :\", f\"{roc_auc_score(y_test, proba_test):.3f}\")\n",
    "print(\"Avg Precision (test) :\", f\"{average_precision_score(y_test, proba_test):.3f}\")\n",
    "\n",
    "# Rapport au seuil OOF\n",
    "res = evaluate_at_threshold(y_test, proba_test, thr_oof)\n",
    "print(f\"\\n=== Rapport @ seuil OOF ({mode}) ===\")\n",
    "print(f\"Seuil: {res['threshold']:.4f}\")\n",
    "print(res[\"report\"])\n",
    "print(\"Confusion matrix [[tn, fp],[fn, tp]]:\\n\", res[\"confusion_matrix\"])\n",
    "\n",
    "# (Option diagnostic) point F1-max sur le TEST (ne PAS geler ce seuil en production)\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_test)\n",
    "P, R, T = prec[:-1], rec[:-1], thr\n",
    "F1 = (2 * P * R) / (P + R + 1e-12)\n",
    "ix = int(np.nanargmax(F1))\n",
    "print(\"\\n[INFO] Test F1-max (diagnostic) : \"\n",
    "      f\"thr={T[ix]:.4f} | P={P[ix]:.3f} R={R[ix]:.3f} F1={F1[ix]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449d4cf",
   "metadata": {},
   "source": [
    "# Courbes Precision–Recall\n",
    "\n",
    "ici trois test de courbes précision recall :\n",
    "- courbe de base sur le test\n",
    "- courbe sur le test avec point selon le choix métier sur OOF ou F1 max\n",
    "- courbe de comparaison train test avec point selon le choix métier sur OOF ou F1 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff6874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR AUC : 0.5602192757077782\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(rec, prec, label=\"PR (test)\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision–Recall (Test)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pr_curve_test_base.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "pr_auc_test = auc(rec, prec)\n",
    "print(\"PR AUC :\", pr_auc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a14a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matplotlib backend: Agg\n",
      "Courbe enregistrée : pr_curve_test.png\n"
     ]
    }
   ],
   "source": [
    "# --- Données PR ---\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba_test)\n",
    "ap = average_precision_score(y_test, proba_test)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "# --- Figure ---\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(rec, prec, label=f\"PR (AP={ap:.3f}, AUC={pr_auc:.3f})\")\n",
    "\n",
    "# Si tu as 'res' (résumé @ seuil OOF) déjà calculé :\n",
    "try:\n",
    "    ax.scatter(res[\"recall\"], res[\"precision\"], s=60, marker=\"o\",\n",
    "               label=f\"Seuil OOF={res['threshold']:.3f}\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Point F1-max sur le test (diagnostic)\n",
    "P, R, T = prec[:-1], rec[:-1], thr\n",
    "F1 = (2*P*R)/(P+R+1e-12)\n",
    "ix = int(np.nanargmax(F1))\n",
    "ax.scatter(R[ix], P[ix], s=80, marker=\"x\", label=f\"F1-max @ {T[ix]:.3f}\")\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision–Recall (Test)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# Sauvegarde + affichage\n",
    "fig.savefig(\"pr_curve_test.png\", dpi=200)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20099ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Switch : afficher/masquer les points F1-max ===\n",
    "SHOW_F1_MAX = True  \n",
    "\n",
    "def _prec_rec_at_thr(y_true, scores, thr):\n",
    "    y_pred = (scores >= thr).astype(int)\n",
    "    p, r, _, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return float(p), float(r)\n",
    "\n",
    "def _f1_max_point(y_true, scores):\n",
    "    \"\"\"Retourne (R*, P*, T*, F1*) au point de F1 maximal sur la courbe PR.\"\"\"\n",
    "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
    "    P, R, T = prec[:-1], rec[:-1], thr\n",
    "    if T.size == 0:\n",
    "        return None  # cas dégénéré\n",
    "    F1 = np.where((P + R) > 0, 2 * P * R / (P + R), 0.0)\n",
    "    ix = int(np.nanargmax(F1))\n",
    "    return float(R[ix]), float(P[ix]), float(T[ix]), float(F1[ix])\n",
    "\n",
    "# --- PR Test ---\n",
    "prec_te, rec_te, thr_te = precision_recall_curve(y_test, proba_test)\n",
    "ap_te  = average_precision_score(y_test, proba_test)\n",
    "auc_te = auc(rec_te, prec_te)\n",
    "\n",
    "# --- PR Train OOF ---\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "proba_tr_oof = cross_val_predict(final_model, X_train, y_train, cv=cv, method=\"predict_proba\")[:, 1]\n",
    "prec_tr, rec_tr, thr_tr = precision_recall_curve(y_train, proba_tr_oof)\n",
    "ap_tr  = average_precision_score(y_train, proba_tr_oof)\n",
    "auc_tr = auc(rec_tr, prec_tr)\n",
    "\n",
    "# --- Plot comparatif ---\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(rec_te, prec_te, label=f\"Test (AP={ap_te:.3f}, AUC={auc_te:.3f})\")\n",
    "ax.plot(rec_tr, prec_tr, linestyle=\"--\", label=f\"Train OOF (AP={ap_tr:.3f}, AUC={auc_tr:.3f})\")\n",
    "\n",
    "# Points @ seuil OOF (si tu utilises le seuil OOF)\n",
    "p_te, r_te = _prec_rec_at_thr(y_test, proba_test, thr_oof)\n",
    "ax.scatter(r_te, p_te, s=60, marker=\"o\", label=f\"Seuil OOF test @ {thr_oof:.3f}\")\n",
    "\n",
    "p_tr, r_tr = _prec_rec_at_thr(y_train, proba_tr_oof, thr_oof)\n",
    "ax.scatter(r_tr, p_tr, s=60, marker=\"s\", label=f\"Seuil OOF train-OOF @ {thr_oof:.3f}\")\n",
    "\n",
    "# ✚ Croix F1-max (affichée seulement si souhaité)\n",
    "if SHOW_F1_MAX:\n",
    "    f1_te = _f1_max_point(y_test, proba_test)\n",
    "    if f1_te is not None:\n",
    "        Rte, Pte, Tte, F1te = f1_te\n",
    "        ax.scatter(Rte, Pte, s=90, marker=\"x\", label=f\"F1-max test @ {Tte:.3f} (F1={F1te:.3f})\")\n",
    "\n",
    "    f1_tr = _f1_max_point(y_train, proba_tr_oof)\n",
    "    if f1_tr is not None:\n",
    "        Rtr, Ptr, Ttr, F1tr = f1_tr\n",
    "        ax.scatter(Rtr, Ptr, s=90, marker=\"D\", label=f\"F1-max train-OOF @ {Ttr:.3f} (F1={F1tr:.3f})\")\n",
    "\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision–Recall : Test vs Train (OOF)\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"pr_curve_train_test_compare.png\", dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2efa448",
   "metadata": {},
   "source": [
    "# Test de l'importance des features \n",
    "\n",
    "Cette partie permet de tester différentes technique pour mettre à jour l'importance des features dans le modèle et ainsi répondre à la question du terrain. \n",
    "Sont testé : permutation importance, feature importance et shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98c81f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation importance (PR AUC) — top 50 :\n",
      "                                  feature  importance_mean  importance_std\n",
      "                    heure_supplementaires         0.191201        0.030905\n",
      "     satisfaction_employee_nature_travail         0.047262        0.016885\n",
      "                           revenu_mensuel         0.038071        0.040469\n",
      "                 nombre_participation_pee         0.038060        0.023016\n",
      "                              departement         0.026003        0.021578\n",
      "                    frequence_deplacement         0.021086        0.014806\n",
      "               note_evaluation_precedente         0.020374        0.012579\n",
      "      satisfaction_employee_environnement         0.018286        0.023224\n",
      "                distance_domicile_travail         0.013308        0.016521\n",
      "                niveau_hierarchique_poste         0.011865        0.005408\n",
      "                           statut_marital         0.010730        0.006777\n",
      "             satisfaction_employee_equipe         0.010303        0.018686\n",
      "      annees_depuis_la_derniere_promotion         0.004042        0.005184\n",
      "                         augmentation_pct         0.004038        0.006652\n",
      "                 note_evaluation_actuelle         0.001222        0.001983\n",
      "                         niveau_education         0.000000        0.000000\n",
      "                                    genre         0.000000        0.000000\n",
      "                                      age         0.000000        0.000000\n",
      "                                    poste         0.000000        0.000000\n",
      "            annes_sous_responsable_actuel         0.000000        0.000000\n",
      "           nombre_experiences_precedentes        -0.000737        0.014550\n",
      "satisfaction_employee_equilibre_pro_perso        -0.001455        0.013001\n",
      "                    nb_formations_suivies        -0.002901        0.008280\n",
      "                            domaine_etude        -0.005024        0.005266\n",
      "                  annee_experience_totale        -0.011513        0.016713\n",
      "                 annees_dans_l_entreprise        -0.013295        0.030239\n",
      "              annees_dans_le_poste_actuel        -0.021684        0.010641\n",
      "                                       feature  importance\n",
      "          cat__departement_Ressources Humaines    0.055203\n",
      "                          num__heure_supp_flag    0.051604\n",
      "                 num__nombre_participation_pee    0.048703\n",
      "                                  num__sat_min    0.045865\n",
      "                       num__revenu_vs_dept_med    0.035973\n",
      "                                 num__sat_mean    0.035438\n",
      "                        num__recent_promo_flag    0.035389\n",
      "           cat__frequence_deplacement_Frequent    0.032764\n",
      "                        num__formations_par_an    0.027971\n",
      "                   cat__departement_Consulting    0.027717\n",
      "                                  num__sat_std    0.026768\n",
      "               num__note_evaluation_precedente    0.026372\n",
      "           num__nombre_experiences_precedentes    0.025097\n",
      "              cat__frequence_deplacement_Aucun    0.025023\n",
      "            cat__domaine_etude_Entrepreunariat    0.023740\n",
      "                   cat__departement_Commercial    0.023703\n",
      "                num__distance_domicile_travail    0.022566\n",
      "                    num__delta_note_evaluation    0.021280\n",
      "                num__nb_annnee_hors_entreprise    0.020695\n",
      "                num__tenure_ratio_current_post    0.020561\n",
      "      num__satisfaction_employee_environnement    0.020389\n",
      "               cat__statut_marital_Célibataire    0.020253\n",
      "num__satisfaction_employee_equilibre_pro_perso    0.019763\n",
      "     num__satisfaction_employee_nature_travail    0.019342\n",
      "      num__annees_depuis_la_derniere_promotion    0.019262\n",
      "                     num__revenu_vs_niveau_med    0.019169\n",
      "                    num__nb_formations_suivies    0.019007\n",
      "                 num__Ecart_nb_annee_sur_poste    0.018570\n",
      "                         num__augmentation_pct    0.018452\n",
      "        cat__frequence_deplacement_Occasionnel    0.018151\n",
      "                  cat__domaine_etude_Marketing    0.017319\n",
      "             num__satisfaction_employee_equipe    0.016753\n",
      "    cat__domaine_etude_Transformation Digitale    0.016724\n",
      "            num__ratio_dans_et_hors_entreprise    0.016338\n",
      "                          num__is_manager_flag    0.016182\n",
      "                cat__statut_marital_Divorcé(e)    0.015773\n",
      "                            bool__sat_low_flag    0.014952\n",
      "                  cat__statut_marital_Marié(e)    0.013926\n",
      "              cat__domaine_etude_Infra & Cloud    0.013021\n",
      "                 num__note_evaluation_actuelle    0.011525\n",
      "                      cat__domaine_etude_Autre    0.011496\n",
      "                        num__long_commute_flag    0.011200\n",
      "        cat__domaine_etude_Ressources Humaines    0.000000\n"
     ]
    }
   ],
   "source": [
    "# importance selon la PR AUC (plus sensible aux positifs rares)\n",
    "perm = permutation_importance(\n",
    "    gs, X_test, y_test,\n",
    "    scoring=\"average_precision\",\n",
    "    n_repeats=20, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "perm_imp = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": X_test.columns,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std\n",
    "    })\n",
    "    .sort_values(\"importance_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\nPermutation importance (PR AUC) — top 50 :\")\n",
    "print(perm_imp.head(50).to_string(index=False))\n",
    "\n",
    "best_pipe = gs.best_estimator_\n",
    "prep = best_pipe.named_steps[\"prep\"]\n",
    "\n",
    "# 2) Récupérer l’estimateur final (adapte 'xgb' au nom réel de ta step)\n",
    "xgb = best_pipe.named_steps[\"xgb\"]  # ou \"clf\" si tu l’as appelée ainsi\n",
    "\n",
    "# 3) Importances \"impurity-based\" de XGBoost (sur les features TRANSFORMÉES)\n",
    "importances = xgb.feature_importances_  # shape = nb de colonnes après preprocess\n",
    "\n",
    "# 4) Récupérer les noms de features transformées depuis le preprocess\n",
    "feat_names = prep.get_feature_names_out(prep.feature_names_in_)\n",
    "\n",
    "import pandas as pd\n",
    "fi = (pd.DataFrame({\"feature\": feat_names, \"importance\": importances})\n",
    "        .sort_values(\"importance\", ascending=False))\n",
    "print(fi.head(100).to_string(index=False))\n",
    "booster = xgb.get_booster()\n",
    "gain_dict = booster.get_score(importance_type=\"gain\")  # dict: nom_feature -> score\n",
    "# Attention : les noms sont du type 'f0','f1',... dans l’ordre des colonnes transformées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc3fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6a8cbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-25 features (|SHAP| moyen) :\n",
      "                                   feature  mean_abs_shap\n",
      "                      num__heure_supp_flag       0.090710\n",
      "                   num__revenu_vs_dept_med       0.081649\n",
      "             num__nombre_participation_pee       0.056102\n",
      "            num__distance_domicile_travail       0.042282\n",
      "       num__nombre_experiences_precedentes       0.039504\n",
      "                             num__sat_mean       0.037430\n",
      "            num__tenure_ratio_current_post       0.036677\n",
      "               cat__departement_Consulting       0.034115\n",
      "            num__nb_annnee_hors_entreprise       0.031935\n",
      "                    num__formations_par_an       0.031616\n",
      "                              num__sat_min       0.030945\n",
      "       cat__frequence_deplacement_Frequent       0.029818\n",
      "                              num__sat_std       0.027890\n",
      " num__satisfaction_employee_nature_travail       0.023095\n",
      "             num__Ecart_nb_annee_sur_poste       0.020789\n",
      "                     num__augmentation_pct       0.018493\n",
      "  num__annees_depuis_la_derniere_promotion       0.015671\n",
      "                 num__revenu_vs_niveau_med       0.015225\n",
      "           num__note_evaluation_precedente       0.015218\n",
      "  num__satisfaction_employee_environnement       0.014549\n",
      "           cat__statut_marital_Célibataire       0.014298\n",
      "                num__nb_formations_suivies       0.013332\n",
      "          cat__frequence_deplacement_Aucun       0.011179\n",
      "cat__domaine_etude_Transformation Digitale       0.010300\n",
      "                      num__is_manager_flag       0.009586\n",
      "\n",
      "Top-20 variables sources (agrégation des dummies par somme |SHAP|) :\n",
      "      source  mean_abs_shap\n",
      "      revenu       0.096874\n",
      "         sat       0.096265\n",
      "      nombre       0.095606\n",
      "       heure       0.090710\n",
      "satisfaction       0.048024\n",
      "          nb       0.045267\n",
      " departement       0.043633\n",
      "   frequence       0.042800\n",
      "    distance       0.042282\n",
      "      tenure       0.036677\n",
      "  formations       0.031616\n",
      "     domaine       0.021024\n",
      "       Ecart       0.020789\n",
      "      statut       0.019339\n",
      "augmentation       0.018493\n",
      "      annees       0.015671\n",
      "        note       0.015298\n",
      "          is       0.009586\n",
      "       delta       0.008610\n",
      "       ratio       0.008504\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================\n",
    "# SHAP sur un Pipeline sklearn (fe -> drop -> prep -> xgb)\n",
    "# ===============================================================\n",
    "\n",
    "# 0) Hypothèse: pipe_XG est DEJA fit, et X_train / X_test existent.\n",
    "\n",
    "# 1) Récupération des étapes utiles\n",
    "fe   = best_pipe.named_steps[\"fe\"]     # ton FeatureEngineer() déjà fit\n",
    "drop = best_pipe.named_steps[\"drop\"]   # ton ColumnDropper() déjà fit\n",
    "prep = best_pipe.named_steps[\"prep\"]   # ton ColumnTransformer déjà fit\n",
    "xgb  = best_pipe.named_steps[\"xgb\"]    # l'estimateur final (XGBClassifier / HistGB / RF ...)\n",
    "\n",
    "# 2) Fonction utilitaire: projeter X brut -> espace features du modèle\n",
    "def to_model_space(X):\n",
    "    X2 = fe.transform(X)             # mêmes features qu'au fit\n",
    "    #X2 = drop.transform(X2)          # mêmes colonnes retirées qu'au fit\n",
    "    Z  = prep.transform(X2)          # encodage + scaling identiques\n",
    "    if sparse.issparse(Z):           # (beaucoup de ploteurs SHAP préfèrent du dense)\n",
    "        Z = Z.toarray()\n",
    "    cols = prep.get_feature_names_out()  # noms post-encodage (scikit-learn >= 1.0)\n",
    "    return pd.DataFrame(Z, columns=cols)\n",
    "\n",
    "# 3) Construire les matrices transformées train/test comme “vues” par le modèle\n",
    "Xtr_df = to_model_space(X_train)\n",
    "Xte_df = to_model_space(X_test)\n",
    "\n",
    "# 4) Explainer SHAP pour modèles d’arbres\n",
    "#    - background = petit échantillon du train transformé (plus rapide/stable)\n",
    "background = shap.sample(Xtr_df, 200, random_state=42)\n",
    "explainer = shap.TreeExplainer(\n",
    "    xgb,\n",
    "    data=background,\n",
    "    feature_perturbation=\"interventional\",   # robuste pour les arbres\n",
    "    model_output=\"probability\"               # SHAP sur l’échelle proba (sinon log-odds)\n",
    ")\n",
    "\n",
    "# 5) Valeurs SHAP sur le TEST (check_additivity=False pour éviter un warning XGB)\n",
    "shap_values = explainer(Xte_df, check_additivity=False)\n",
    "\n",
    "# 6) Plots globaux/local (adapter max_display au besoin)\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")               # backend non interactif (sécurise les exports)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "os.makedirs(\"visualisations\", exist_ok=True)\n",
    "\n",
    "# 1) BEESWARM (global) — utiliser la fonction legacy, très fiable à l’export\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(\n",
    "    shap_values.values if hasattr(shap_values, \"values\") else shap_values,\n",
    "    Xte_df,\n",
    "    feature_names=getattr(Xte_df, \"columns\", None),\n",
    "    plot_type=\"dot\",\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visualisations/beeswarm.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 2) BAR (global) — importance moyenne |SHAP|\n",
    "plt.figure(figsize=(8, 6))\n",
    "shap.summary_plot(\n",
    "    shap_values.values if hasattr(shap_values, \"values\") else shap_values,\n",
    "    Xte_df,\n",
    "    feature_names=getattr(Xte_df, \"columns\", None),\n",
    "    plot_type=\"bar\",\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"visualisations/summary_bar.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# 3) WATERFALL (local) — nouvelle API, avec fallback legacy si besoin\n",
    "i = 0  # l'individu que tu veux tracer\n",
    "try:\n",
    "    # nouvelle API (si dispo dans ta version de shap)\n",
    "    shap.plots.waterfall(shap_values[i], max_display=20, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualisations/waterfall.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "except Exception:\n",
    "    # Fallback legacy 100% matplotlib\n",
    "    base_value = (\n",
    "        shap_values.base_values[i]\n",
    "        if hasattr(shap_values, \"base_values\")\n",
    "        else (explainer.expected_value if np.isscalar(explainer.expected_value) else explainer.expected_value[1])\n",
    "    )\n",
    "    vals = shap_values.values[i] if hasattr(shap_values, \"values\") else shap_values[i]\n",
    "    shap.plots._waterfall.waterfall_legacy(\n",
    "        base_value,\n",
    "        vals,\n",
    "        feature_names=getattr(Xte_df, \"columns\", None),\n",
    "        features=Xte_df.iloc[i] if hasattr(Xte_df, \"iloc\") else None,\n",
    "        max_display=20,\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"visualisations/waterfall.png\", format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# 7) (Option) Exporter l’importance moyenne absolue par feature\n",
    "imp_df = (\n",
    "    pd.DataFrame({\n",
    "        \"feature\": Xte_df.columns,\n",
    "        \"mean_abs_shap\": np.abs(shap_values.values).mean(axis=0)\n",
    "    })\n",
    "    .sort_values(\"mean_abs_shap\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop-25 features (|SHAP| moyen) :\")\n",
    "print(imp_df.head(25).to_string(index=False))\n",
    "\n",
    "# 8) (Option) Agréger par variable source (avant OHE) si tu utilises un schéma \"num__\" / \"cat__\"\n",
    "#    Regroupe toutes les dummies d’une même variable d’origine (somme des |SHAP|)\n",
    "def source_from_feature_name(name: str) -> str:\n",
    "    # Exemple d’extractions: \"num__age\" -> \"age\" ; \"cat__poste_Manager\" -> \"poste\"\n",
    "    if name.startswith(\"num__\"):\n",
    "        return name.split(\"num__\", 1)[1].split(\"_\", 1)[0] if \"__\" in name else name[5:]\n",
    "    if name.startswith(\"cat__\"):\n",
    "        base = name.split(\"cat__\", 1)[1]\n",
    "        return base.split(\"_\", 1)[0]   # avant le premier \"_\", ex: \"poste_Manager\" -> \"poste\"\n",
    "    return name\n",
    "\n",
    "agg_df = (\n",
    "    imp_df.assign(source=imp_df[\"feature\"].map(source_from_feature_name))\n",
    "          .groupby(\"source\", as_index=False)[\"mean_abs_shap\"].sum()\n",
    "          .sort_values(\"mean_abs_shap\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop-20 variables sources (agrégation des dummies par somme |SHAP|) :\")\n",
    "print(agg_df.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b8358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Helper: dependence plot robuste sur un nom de feature transformée ---\n",
    "def shap_dependence(name, color=None, max_display=None):\n",
    "    assert name in Xte_df.columns, f\"{name} absent de Xte_df.columns\"\n",
    "    if color is not None and color not in Xte_df.columns:\n",
    "        color = None  # si pas trouvé, on ignore la couleur\n",
    "    shap.plots.scatter(\n",
    "        shap_values[:, name],\n",
    "        color=shap_values[:, color] if color else None,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP dependence — {name}\" + (f\" (color: {color})\" if color else \"\"))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "top6 = [\n",
    "    (\"num__revenu_vs_dept_med\",       \"num__revenu_vs_niveau_med\"),   # effet rémunération relative\n",
    "    (\"num__heure_supp_flag\",          \"num__sat_mean\"),               # surcharge vs satisfaction\n",
    "    (\"num__distance_domicile_travail\",\"cat__frequence_deplacement_Frequent\"),\n",
    "    (\"num__nombre_experiences_precedentes\",\"num__formations_par_an\"),\n",
    "    (\"num__tenure_ratio_current_post\",\"num__Ecart_nb_annee_sur_poste\"),\n",
    "    (\"num__sat_mean\",                 \"num__sat_std\"),\n",
    "]\n",
    "\n",
    "for feat, color in top6:\n",
    "    shap_dependence(feat, color=color)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Projet 4 (Poetry)",
   "language": "python",
   "name": "projet4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
